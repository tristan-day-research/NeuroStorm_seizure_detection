{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "This project assumes GitHub and Google Cloud Storage info is stored in environmental variables.  \n",
        "It was developed in Google Colab, but can be run locally by selecting 'local' as the environment.  \n",
        "\n",
        "For setup details (including required environment variables), see the README.md in the GitHub repository:  \n",
        "https://github.com/tristan-day-research/NeuroStorm_seizure_detection  "
      ],
      "metadata": {
        "id": "PyY_pxNWgXLq"
      },
      "id": "PyY_pxNWgXLq"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP GITHUB AND GCP ENVIRONMENT VARIABLES ---\n",
        "# Ensure the following environmental variables are set in Colab user data:\n",
        "# - GITHUB_PAT: GitHub Personal Access Token\n",
        "# - GITHUB_EMAIL: GitHub email for commits\n",
        "# - GITHUB_USER_NAME: GitHub username\n",
        "# - GCP_EEG_PROJECT_ID: Google Cloud Project ID\n",
        "# - GCP_EEG_BUCKET_NAME: (Optional) GCP bucket for EEG data\n",
        "\n",
        "# Select environment\n",
        "ENVIRONMENT = 'colab'   # Choose 'local' or 'colab'\n",
        "BRANCH_NAME = 'main'\n",
        "\n",
        "if ENVIRONMENT == 'colab':\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Retrieve GitHub credentials from Colab user data\n",
        "    token = userdata.get('GITHUB_PAT')\n",
        "    github_email = userdata.get('GITHUB_EMAIL')\n",
        "    github_username = userdata.get('GITHUB_USER_NAME')\n",
        "\n",
        "    # Clone the repository (done here as the helper file isn't available yet)\n",
        "    !git clone -b {BRANCH_NAME} https://{token}@github.com/tristan-day-research/NeuroStorm_seizure_detection.git\n",
        "\n",
        "    # Change to correct directory\n",
        "    %cd /content/NeuroStorm_seizure_detection/\n",
        "\n",
        "    # Load the helper file now that the repo is cloned\n",
        "    from src.setup import configure_environment\n",
        "\n",
        "    # Run full environment setup\n",
        "    bucket_name = configure_environment(environment=ENVIRONMENT)\n"
      ],
      "metadata": {
        "id": "wFLY5da1Mw8v",
        "outputId": "a064db55-70b5-426b-a02f-37afe7f76388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wFLY5da1Mw8v",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NeuroStorm_seizure_detection'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 164 (delta 75), reused 151 (delta 66), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (164/164), 1.91 MiB | 7.65 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/NeuroStorm_seizure_detection\n",
            "GCP Project Set\n",
            "Git configured with your user data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import threading\n",
        "import subprocess\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from warnings import warn\n",
        "\n",
        "# Third-Party Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Machine Learning/Deep Learning Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torch.nn.init import trunc_normal_\n",
        "from torch.nn.utils import clip_grad_norm_, rnn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "PG7sy9nPeNHB",
        "outputId": "2a900c2b-25a2-4d93-c636-fe09d2923f9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PG7sy9nPeNHB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "Each EEG raw signal is segemnted into patches of a fixed length. Fast Fourier Transform (FFT) spectra are made from these patches which will be used to train the Vector-Quantized Variational Autoencoder (VQ-VAE)."
      ],
      "metadata": {
        "id": "3fE92ADGXluH"
      },
      "id": "3fE92ADGXluH"
    },
    {
      "cell_type": "code",
      "source": [
        "# bucket_name = os.getenv('GCP_EEG_BUCKET_NAME')\n",
        "!echo $GCP_EEG_BUCKET_NAME\n",
        "\n",
        "print(bucket_name)"
      ],
      "metadata": {
        "id": "kbj8xSZBbSF_",
        "outputId": "6119a40c-0ad9-4e81-eae3-d99257f2e768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kbj8xSZBbSF_",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from src.data_and_FFT import EEGDataset\n",
        "\n",
        "# Visualization Function (Separate from Data Loader)\n",
        "def visualize_eeg_and_fft(eeg_tensor, fft_tensor, fft_size, sample_idx=0):\n",
        "    raw_signal = eeg_tensor[sample_idx, :, 0].cpu().numpy()  # [patch_size]\n",
        "    fft_signal = fft_tensor[sample_idx, :, 0].cpu().numpy()  # [fft_size // 2 + 1]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(raw_signal)\n",
        "    plt.title(\"Raw EEG Signal (Time Domain)\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(fft_signal)\n",
        "    plt.title(\"FFT of EEG Signal (Frequency Domain)\")\n",
        "    plt.xlabel(\"Frequency Bin\")\n",
        "    plt.ylabel(\"Magnitude\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Parameters\n",
        "file_prefix = 'kaggle/input/hms-harmful-brain-activity-classification'\n",
        "patch_size = 200\n",
        "overlap = 50\n",
        "fft_size = 256\n",
        "\n",
        "\n",
        "# Load Dataset and Visualize\n",
        "def load_and_visualize_samples(bucket_name, file_prefix, patch_size, overlap, fft_size):\n",
        "    dataset = EEGDataset(\n",
        "        bucket_name=bucket_name,\n",
        "        file_prefix=file_prefix,\n",
        "        patch_size=patch_size,\n",
        "        overlap=overlap,\n",
        "        fft_size=fft_size\n",
        "    )\n",
        "\n",
        "    print(f\"Found {len(dataset)} files. Visualizing...\")\n",
        "\n",
        "    for i in range(min(3, len(dataset))):  # Visualize 3 samples or fewer\n",
        "        fft_data, mask = dataset[i]\n",
        "        visualize_eeg_and_fft(fft_data, fft_data, fft_size)\n",
        "\n",
        "\n",
        "# Run Test\n",
        "load_and_visualize_samples(\n",
        "    bucket_name=bucket_name,\n",
        "    file_prefix=file_prefix,\n",
        "    patch_size=patch_size,\n",
        "    overlap=overlap,\n",
        "    fft_size=fft_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "HN4ytOL4Ypsw",
        "outputId": "a85d6ad4-d986-433c-908f-53bffdd9ac66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "HN4ytOL4Ypsw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot determine path without bucket name.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-544906000317>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Run Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m load_and_visualize_samples(\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-544906000317>\u001b[0m in \u001b[0;36mload_and_visualize_samples\u001b[0;34m(bucket_name, file_prefix, patch_size, overlap, fft_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Load Dataset and Visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_visualize_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfft_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     dataset = EEGDataset(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/NeuroStorm_seizure_detection/src/data_and_FFT.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bucket_name, file_prefix, patch_size, overlap, fft_size, stride)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/bucket.py\u001b[0m in \u001b[0;36mlist_blobs\u001b[0;34m(self, max_results, page_token, prefix, delimiter, start_offset, end_offset, include_trailing_delimiter, versions, projection, fields, client, timeout, retry, match_glob, include_folders_as_prefixes, soft_deleted, page_size)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \"\"\"\n\u001b[1;32m   1482\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_require_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m         return client.list_blobs(\n\u001b[0m\u001b[1;32m   1484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/client.py\u001b[0m in \u001b[0;36mlist_blobs\u001b[0;34m(self, bucket_or_name, max_results, page_token, prefix, delimiter, start_offset, end_offset, include_trailing_delimiter, versions, projection, fields, page_size, timeout, retry, match_glob, include_folders_as_prefixes, soft_deleted)\u001b[0m\n\u001b[1;32m   1396\u001b[0m             \u001b[0mextra_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userProject\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/o\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         iterator = self._list_resource(\n\u001b[1;32m   1400\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/bucket.py\u001b[0m in \u001b[0;36mpath\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;34m\"\"\"The URL path to this bucket.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot determine path without bucket name.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot determine path without bucket name."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "xwTX_KCeklYj"
      },
      "id": "xwTX_KCeklYj"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.vqvae.train import train, validate\n",
        "from src.vqvae.data import EEGDataset, ToPatches\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "stride = 150\n",
        "batch_size = 4\n",
        "num_workers = 4\n",
        "num_epochs = 7\n",
        "lr = 1e-4\n",
        "lr_scheduler_step_size = 1\n",
        "lr_scheduler_gamma = 0.9\n",
        "accumulation_steps = 2\n",
        "\n",
        "model_name = \"2025_VQVAE_v1\"\n",
        "codebook_size = 1024\n",
        "emb_dim = 64\n",
        "\n",
        "# --- Dataset and Loader ---\n",
        "transform_to_patches = ToPatches(patch_size=200, stride=stride)\n",
        "eeg_dataset = EEGDataset(bucket_name=BUCKET_NAME, blob_prefix=\"train_eegs_HMS_processed\",\n",
        "                         transform=transform_to_patches)\n",
        "train_size = int(0.8 * len(eeg_dataset))\n",
        "valid_size = len(eeg_dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(eeg_dataset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# --- Model, Optimizer, Scheduler ---\n",
        "model_class = globals()[model_name]\n",
        "model = model_class(codebook_size=codebook_size, emb_dim=emb_dim)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "scheduler = StepLR(optimizer, step_size=lr_scheduler_step_size, gamma=lr_scheduler_gamma)\n",
        "\n",
        "loss_function = partial(fft_masked_mse_loss, phase_start_batch=200, phase_end_batch=400)\n",
        "\n",
        "# --- Train and Validate ---\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    avg_loss = train(model, train_loader, optimizer, loss_function, device,\n",
        "                     scheduler=scheduler, accum_steps=accumulation_steps)\n",
        "    print(f\"Train Loss: {avg_loss:.4f}\")\n",
        "    val_loss = validate(model, valid_loader, loss_function, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "#\n"
      ],
      "metadata": {
        "id": "90HfgVo8knvO",
        "outputId": "c7a327ab-af26-44bd-9886-8a4028485365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "id": "90HfgVo8knvO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BUCKET_NAME' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d494cc7f2bed>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# --- Dataset and Loader ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtransform_to_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m eeg_dataset = EEGDataset(bucket_name=BUCKET_NAME, blob_prefix=\"train_eegs_HMS_processed\",\n\u001b[0m\u001b[1;32m     23\u001b[0m                          transform=transform_to_patches)\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BUCKET_NAME' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oeu2dFIGRe-I"
      },
      "id": "Oeu2dFIGRe-I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}