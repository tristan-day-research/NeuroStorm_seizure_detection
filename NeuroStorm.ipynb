{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "This project assumes GitHub and Google Cloud Storage info is stored in environmental variables.  \n",
        "It was developed in Google Colab, but can be run locally by selecting 'local' as the environment.  \n",
        "\n",
        "For setup details (including required environment variables), see the README.md in the GitHub repository:  \n",
        "https://github.com/tristan-day-research/NeuroStorm_seizure_detection  "
      ],
      "metadata": {
        "id": "PyY_pxNWgXLq"
      },
      "id": "PyY_pxNWgXLq"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP GITHUB AND GCP ENVIRONMENT VARIABLES ---\n",
        "# Ensure the following environmental variables are set in Colab user data:\n",
        "# - GITHUB_PAT: GitHub Personal Access Token\n",
        "# - GITHUB_EMAIL: GitHub email for commits\n",
        "# - GITHUB_USER_NAME: GitHub username\n",
        "# - GCP_EEG_PROJECT_ID: Google Cloud Project ID\n",
        "# - GCP_EEG_BUCKET_NAME: (Optional) GCP bucket for EEG data\n",
        "\n",
        "# Select environment\n",
        "ENVIRONMENT = 'colab'   # Choose 'local' or 'colab'\n",
        "BRANCH_NAME = 'main'\n",
        "\n",
        "if ENVIRONMENT == 'colab':\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Retrieve GitHub credentials from Colab user data\n",
        "    token = userdata.get('GITHUB_PAT')\n",
        "    github_email = userdata.get('GITHUB_EMAIL')\n",
        "    github_username = userdata.get('GITHUB_USER_NAME')\n",
        "\n",
        "    # Clone the repository (done here as the helper file isn't available yet)\n",
        "    !git clone -b {BRANCH_NAME} https://{token}@github.com/tristan-day-research/NeuroStorm_seizure_detection.git\n",
        "\n",
        "    # Change to correct directory\n",
        "    %cd /content/NeuroStorm_seizure_detection/\n",
        "\n",
        "    # Load the helper file now that the repo is cloned\n",
        "    from src.setup import configure_environment\n",
        "\n",
        "    # Run full environment setup\n",
        "    bucket_name = configure_environment(environment=ENVIRONMENT)\n"
      ],
      "metadata": {
        "id": "wFLY5da1Mw8v"
      },
      "id": "wFLY5da1Mw8v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import threading\n",
        "import subprocess\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from warnings import warn\n",
        "\n",
        "# Third-Party Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Machine Learning/Deep Learning Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torch.nn.init import trunc_normal_\n",
        "from torch.nn.utils import clip_grad_norm_, rnn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "PG7sy9nPeNHB",
        "outputId": "8f2035e6-09cf-46d7-b254-c8422e76b2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PG7sy9nPeNHB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "Each EEG raw signal is segemnted into patches of a fixed length. Fast Fourier Transform (FFT) spectra are made from these patches which will be used to train the Vector-Quantized Variational Autoencoder (VQ-VAE)."
      ],
      "metadata": {
        "id": "3fE92ADGXluH"
      },
      "id": "3fE92ADGXluH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "xwTX_KCeklYj"
      },
      "id": "xwTX_KCeklYj"
    },
    {
      "cell_type": "code",
      "source": [
        "from src.vqvae.train import train, validate\n",
        "from src.vqvae.data import EEGDataset, ToPatches\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "stride = 150\n",
        "batch_size = 4\n",
        "num_workers = 4\n",
        "num_epochs = 7\n",
        "lr = 1e-4\n",
        "lr_scheduler_step_size = 1\n",
        "lr_scheduler_gamma = 0.9\n",
        "accumulation_steps = 2\n",
        "\n",
        "model_name = \"2025_VQVAE_v1\"\n",
        "codebook_size = 1024\n",
        "emb_dim = 64\n",
        "\n",
        "# --- Dataset and Loader ---\n",
        "transform_to_patches = ToPatches(patch_size=200, stride=stride)\n",
        "eeg_dataset = EEGDataset(bucket_name=BUCKET_NAME, blob_prefix=\"train_eegs_HMS_processed\",\n",
        "                         transform=transform_to_patches)\n",
        "train_size = int(0.8 * len(eeg_dataset))\n",
        "valid_size = len(eeg_dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(eeg_dataset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# --- Model, Optimizer, Scheduler ---\n",
        "model_class = globals()[model_name]\n",
        "model = model_class(codebook_size=codebook_size, emb_dim=emb_dim)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "scheduler = StepLR(optimizer, step_size=lr_scheduler_step_size, gamma=lr_scheduler_gamma)\n",
        "\n",
        "loss_function = partial(fft_masked_mse_loss, phase_start_batch=200, phase_end_batch=400)\n",
        "\n",
        "# --- Train and Validate ---\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    avg_loss = train(model, train_loader, optimizer, loss_function, device,\n",
        "                     scheduler=scheduler, accum_steps=accumulation_steps)\n",
        "    print(f\"Train Loss: {avg_loss:.4f}\")\n",
        "    val_loss = validate(model, valid_loader, loss_function, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "#\n"
      ],
      "metadata": {
        "id": "90HfgVo8knvO",
        "outputId": "c7a327ab-af26-44bd-9886-8a4028485365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "id": "90HfgVo8knvO",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BUCKET_NAME' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d494cc7f2bed>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# --- Dataset and Loader ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtransform_to_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m eeg_dataset = EEGDataset(bucket_name=BUCKET_NAME, blob_prefix=\"train_eegs_HMS_processed\",\n\u001b[0m\u001b[1;32m     23\u001b[0m                          transform=transform_to_patches)\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BUCKET_NAME' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oeu2dFIGRe-I"
      },
      "id": "Oeu2dFIGRe-I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}